{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Types Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "\n",
    "# Read the data from directory\n",
    "df = pl.read_parquet(\"../0 - Data/1 - merge/merged_transactions.pq\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Combine `Year`, `Month`, `Day`, and `Time` into a single `Datetime` column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.with_columns([\n",
    "    # Ensure 'Year', 'Month', 'Day' are numeric first\n",
    "    pl.col('Year').cast(pl.Int32).alias('Year'),\n",
    "    pl.col('Month').cast(pl.Int32).alias('Month'),\n",
    "    pl.col('Day').cast(pl.Int32).alias('Day'),\n",
    "    \n",
    "    # Ensure 'Time' is string for concatenation\n",
    "    pl.col('Time').cast(pl.Utf8).alias('Time'),\n",
    "\n",
    "    # Create a new 'Datetime_str' column by concatenating 'Year', 'Month', 'Day', and 'Time' as a string\n",
    "    pl.concat_str(\n",
    "        [pl.col(\"Year\").cast(pl.Utf8), \n",
    "         pl.col(\"Month\").cast(pl.Utf8).str.zfill(2), \n",
    "         pl.col(\"Day\").cast(pl.Utf8).str.zfill(2), \n",
    "         pl.col(\"Time\")],\n",
    "        separator=\"-\"\n",
    "    ).alias(\"Datetime_str\")\n",
    "])\n",
    "\n",
    "# Parse the concatenated 'Datetime_str' into a proper Datetime column\n",
    "df = df.with_columns([\n",
    "    pl.col(\"Datetime_str\").str.strptime(pl.Datetime, format=\"%Y-%m-%d-%H:%M\").alias(\"Datetime\")\n",
    "])\n",
    "\n",
    "# Drop unnecessary columns: Time and temporary Datetime_str\n",
    "df = df.drop([\"Time\", \"Datetime_str\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Handle Rest of the Date Columns\n",
    "\n",
    "- Expire Column\n",
    "- Acc Open Date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to date\n",
    "df = df.with_columns(\n",
    "        (\n",
    "            pl.col(\"Expires\")\n",
    "            .str.strptime(pl.Date, format=\"%m/%Y\", strict=False)\n",
    "            .alias(\"Expires\")\n",
    "        )\n",
    "    )\n",
    "\n",
    "# Convert to date\n",
    "df = df.with_columns(\n",
    "        (\n",
    "            pl.col(\"Acct Open Date\")\n",
    "            .str.strptime(pl.Date, format=\"%m/%Y\", strict=False)\n",
    "            .alias(\"Acct Open Date\")\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Handle Financial Columns\n",
    "\n",
    "- Amount\n",
    "- Credit Limit\n",
    "- Yearly Incom - Person\n",
    "- Total Debt\n",
    "- Per Capita Income - Zipcode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "\n",
    "# Define the financial columns that need cleaning\n",
    "financial_columns = [\n",
    "    \"Amount\", \"Credit Limit\", \"Yearly Income - Person\", \n",
    "    \"Total Debt\", \"Per Capita Income - Zipcode\"\n",
    "]\n",
    "\n",
    "# Apply the transformations in a loop\n",
    "df = df.with_columns([\n",
    "    # Clean each financial column in the list\n",
    "    pl.when(pl.col(col).is_not_null())\n",
    "      .then(pl.col(col).str.replace('$', '', literal=True))\n",
    "      .otherwise(None)\n",
    "      .cast(pl.Float64)\n",
    "      .alias(col)\n",
    "    for col in financial_columns\n",
    "] + [\n",
    "    # Cast other columns directly\n",
    "    pl.col(\"FICO Score\").cast(pl.Int64).alias(\"FICO Score\"),\n",
    "    pl.col(\"Num Credit Cards\").cast(pl.Int64).alias(\"Num Credit Cards\")\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Handle Boolean Columns\n",
    "\n",
    "- Is Fraud?\n",
    "- Has Chip\n",
    "- Card on Dark Web"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert columns 'Is Fraud?', 'Has Chip', 'Card on Dark Web' to boolean\n",
    "df = df.with_columns([\n",
    "    pl.when(pl.col(\"Is Fraud?\") == \"Yes\").then(1).otherwise(0).alias(\"Is Fraud\"),\n",
    "    pl.when(pl.col(\"Has Chip\") == \"YES\").then(1).otherwise(0).alias(\"Has Chip\"),\n",
    "    pl.when(pl.col(\"Card on Dark Web\") == \"Yes\").then(1).otherwise(0).alias(\"Card on Dark Web\")\n",
    "])\n",
    "\n",
    "df = df.drop([ \"Is Fraud?\" ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Categorical Columns (UTF8)\n",
    "\n",
    "- Merchant Name\n",
    "- Card Type\n",
    "- Card Brand\n",
    "- Merchant City\n",
    "- Merchant State\n",
    "- Zip\n",
    "- Use Chip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure categorical or string columns are UTF8 type\n",
    "df = df.with_columns([\n",
    "    pl.col(\"Merchant Name\").cast(pl.Utf8).alias(\"Merchant Name\"),\n",
    "    pl.col(\"Card Brand\").cast(pl.Utf8).alias(\"Card Brand\"),\n",
    "    pl.col(\"Card Type\").cast(pl.Utf8).alias(\"Card Type\"),\n",
    "    pl.col(\"Merchant City\").cast(pl.Utf8).alias(\"Merchant City\"),\n",
    "    pl.col(\"Merchant State\").cast(pl.Utf8).alias(\"Merchant State\"),\n",
    "    pl.col(\"Zip\").cast(pl.Utf8).alias(\"Zip\"),\n",
    "    pl.col(\"Use Chip\").cast(pl.Utf8).alias(\"Use Chip\")\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Handle Rest of the Numerical Columns (Float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure 'Latitude' and 'Longitude' are float type\n",
    "df = df.with_columns([\n",
    "    pl.col(\"Latitude\").cast(pl.Float64).alias(\"Latitude\"),\n",
    "    pl.col(\"Longitude\").cast(pl.Float64).alias(\"Longitude\")\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Handle Null Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find all the nulls within the data\n",
    "def check_for_null_cols(df: pl.DataFrame):\n",
    "    null_counts = df.null_count()\n",
    "    null_cols = []\n",
    "    for idx, col in enumerate(null_counts):\n",
    "        if col[0] > 0:\n",
    "            null_cols.append(null_counts.columns[idx])\n",
    "\n",
    "    return null_cols\n",
    "\n",
    "print(\"Columns with NULL values:\", check_for_null_cols(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill null values with appropriate defaults\n",
    "df = df.with_columns([\n",
    "    pl.col(\"Zip\").fill_null(\"Unknown\"),\n",
    "    pl.col(\"Errors?\").fill_null(0),\n",
    "    pl.col(\"Merchant State\").fill_null(\"Unknown\"),\n",
    "    pl.col(\"Apartment\").fill_null(\"Unknown\")\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Just to be sure\n",
    "print(\"Columns with NULL values:\", check_for_null_cols(df))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Type Checked Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save the cleaned data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "clean_dir = \"../0 - Data/2 - clean\"\n",
    "if not os.path.exists(clean_dir):\n",
    "    os.makedirs(clean_dir)\n",
    "\n",
    "# Type checked data but with full columns (For further data exploration)\n",
    "df.write_parquet(\"../0 - Data/2 - clean/clean_transactions.pq\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
